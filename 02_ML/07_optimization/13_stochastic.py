# Batch Gradient Descend        -> uso tutto il training set
#   -> high memory
#   -> no updates -> rerun everything
# Stochastic Gradient Descend   -> uso un elemento per volta del training set
#   -> noise
#   -> quick to update
# Mini Batch Gradient Descend   -> uso un sottoinsieme alla volta del training set
